{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS Applied to data science\n",
    "\n",
    "## Exercises PART 1: Descriptive statistics and data exploration\n",
    "\n",
    "Employing descriptive statistics is one of the main steps of the POC stage (proof-of-concept) and extremely helpful during model evaluation.  \n",
    "A sound knowledge of statistics will help you design your machine learning experiments and interpret the results easily.   \n",
    "In this notebook you'll find some common routines for descriptive statistics in Python, and exercises about data transformation and scaling. \n",
    "\n",
    "![Image](../images/data_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from numpy.random import seed, randn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "#%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# jupyter lab configs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# precision options\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "%precision 4\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Write your own summary statistics and descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement code for the functions below. In each function, make sure you call the function written before. E.g., in `my_rmse()` use the values returned by `my_mse()`. The aim of this exercise is just to understand how these diferent metrics are related, and which aspect of the data they are representing.   \n",
    "\n",
    "**You can use the map below to see the relationships between metrics and then plan how to structure your functions** \n",
    "\n",
    "![Image](../images/map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mean(x):\n",
    "    if len(x)>0:\n",
    "        return sum(x)/len(x)\n",
    "\n",
    "def my_sum_squares():\n",
    "    pass\n",
    "\n",
    "def my_mse():\n",
    "    # mean squared error\n",
    "    pass\n",
    "\n",
    "def my_rmse():\n",
    "    # rooted mean squared error\n",
    "    pass\n",
    "\n",
    "def my_variance():\n",
    "    pass\n",
    "\n",
    "def my_std_dev():\n",
    "    pass\n",
    "\n",
    "def my_std_error():\n",
    "    pass\n",
    "\n",
    "def my_confidence_95():\n",
    "    pass\n",
    "    \n",
    "def my_covariance():\n",
    "    pass\n",
    "\n",
    "def my_coeficient_variation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure it works!! In Python use `assert`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.randint(500, size=(32))\n",
    "assert my_mean(x) == np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. Practice data description and summarization with pandas\n",
    "\n",
    "### Here's a collection of `pandas` functions I find most useful during the data exploration stage:\n",
    "* `.describe()`  and `.describe(include=np.object)` \n",
    "* `.info()`\n",
    "* `.unique()` and `.nunique()`\n",
    "* `.count_values()`\n",
    "* `.group_by().agg()`\n",
    "* `.pd.cut()` and `pd.qcut()` for binning continuous vars into discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "from sklearn.datasets import load_boston\n",
    "dt = load_boston(return_X_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOSTON DATASET  \n",
    "**TARGET**  \n",
    "`MEDV` Median value of owner-occupied homes in thousands\n",
    "\n",
    "**POSSIBLE FACTORS**  \n",
    "`CRIM` per capita crime rate by town    \n",
    "`ZN` proportion of residential land zoned for lots over 25,000 sq.ft.    \n",
    "`INDUS` proportion of non-retail business acres per town  \n",
    "`CHAS` Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)   \n",
    "`NOX` nitric oxides concentration (parts per 10 million)   \n",
    "`RM` average number of rooms per dwelling      \n",
    "`AGE` proportion of owner-occupied units built prior to 1940    \n",
    "`DIS` weighted distances to five Boston employment centres       \n",
    "`RAD` index of accessibility to radial highways    \n",
    "`TAX` full-value property-tax rate per $10,000     \n",
    "`PTRATIO` pupil-teacher ratio by town  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston house prices data - CONTINUOUS DATA\n",
    "dt = load_boston(return_X_y=False)\n",
    "df = pd.DataFrame(data = np.c_[dt['data'],dt['target']])\n",
    "df.columns = np.append(dt['feature_names'], 'MED_VALUE')\n",
    "df.drop(['B', 'LSTAT'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas `describe()` for continuous data and `describe(include=np.object)` for categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of unique values per variable to understand which are continuous and which are discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(c, 'has',  df[c].nunique(), 'unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the table above and pay attention to the continuous variables you identified.   \n",
    "Just looking at the relationship between the **mean** and **std**, which variables seem to be normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which seem to be not normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DIS.plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some categories in the data using `pd.cut()`.     \n",
    "Check the variable `NOX` that indicates a measure of pollution.    \n",
    "How many categories could we extract from this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NOX.plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.48, 0.58, 0.68, 0.78, 1]\n",
    "labels = ['level1', 'level2', 'level3', 'level4', 'level5']\n",
    "df['NOX_categories'] = pd.cut(df['NOX'], labels=labels, bins=bins)\n",
    "df[['NOX', 'NOX_categories']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the new variable `NOX_categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NOX_categories.describe(include=np.object)\n",
    "df.NOX_categories.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with probability distributions\n",
    "\n",
    "Examples using the **stats** module of **scipy**:  \n",
    "\n",
    "Probability function: `stats.poisson.pmf()`  \n",
    "Cumulative function: `stats.poisson.cdf()`  \n",
    "Generate samples: `stats.poisson.rvs(3, size=5000)`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of items per order follows a poisson distribution with lambda = 2.  \n",
    "What is the probability of having an order with exactly 6 items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.poisson.pmf(6, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of clicks per add follows a poisson distribution with lambda = 10.  \n",
    "What is the probability of having 15 clicks in one add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.poisson.pmf(15, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three ways to check the distribution of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can show the frequency as absolute values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CRIM.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...of you can show as percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CRIM.plot.hist(bins=10, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use hypothesis tests\n",
    "\n",
    "A common way of testing if a variable has a normal distribution is to use the **Shapiro-Wilk Test**.        \n",
    "In this test, the null hypothesis is that the data comes from a normal distribution. When **p < 0.05** we can reject this hypothesis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the test from scipy\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# create a variable by drawing from a normal distribution\n",
    "normal_data = np.random.normal(8, 3.3, 100)\n",
    "# apply the test, which returns the statistic and the p-value\n",
    "shapiro(normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is > 0.05 (by far), so what do we do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now repeat the test using one of the dataset's variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this variable normally distributed?  \n",
    "Try it yourself using the other variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro(df.RM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use QQ-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# example of a normally distributed variable\n",
    "p = qqplot(normal_data, line='s')\n",
    "\n",
    "\n",
    "# example of a variable approaches normality but has outliers\n",
    "q = qqplot(df.RM, line='s')\n",
    "\n",
    "# example of a very not-normally distributed variable\n",
    "r = qqplot(df.CRIM, line='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformations\n",
    "\n",
    "## The most common procedures are *feature scaling* and *linearization*:\n",
    "\n",
    "1. `Feature scaling` means you transform the data so all quantitative features are, let's say, *speaking the same language*.   \n",
    "Common scaling techniques are:\n",
    "* Min-max (a.k.a. **normalization**)\n",
    "* z-score (a.k.a. **standardization**)  \n",
    "\n",
    "Particularly, I always use z-score, and this transformation is also the most common method employed in *unsupervised learning* such as PCA, clustering, etc.\n",
    "\n",
    "2. `Linearization` will be usually needed to transform the `target`, or `dependent` variable, i. e., what you are trying to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling (a.k.a. standardization, normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](../images/zscore.gif)   \n",
    "\n",
    "\n",
    "\n",
    "You can use `scipy.stats.zscore()` or write your own function, which is way more fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_z_score(data):\n",
    "    \"\"\" Applies z-score transformation to a vector\"\"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data and check the mean and sd before transformation\n",
    "data = randn(5)\n",
    "np.mean(data), np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check what happens to the mean and std after the z-score transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = my_z_score(data)\n",
    "np.mean(data_std), np.std(data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearization\n",
    "## *Dealing with non-gaussian data* \n",
    "\n",
    "There's usually four ways of carrying on the analysis if you are working with regression problems and quantitative **target** variables that are not normally-distributed.\n",
    "1. Look for models that don't need linear relationships in the data (E. g. random forests, boosted trees)\n",
    "2. Look for models that can handle different distributions, like Poisson or Binomial (a.k.a. Generalized Linear Models)\n",
    "3. If you are using a hypothesis test, use bootstrapping to generate to generate the null model \n",
    "4. Apply transformations (log, sqrt, box-cox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!**  \n",
    "\n",
    "Log-transformation is a common tool in statistics. However, there is a pitfall in using log transformation of your data.  \n",
    "Especially if you have a wide numerical range in a feature, keep in mind that log will \"compress\" the data significantly more, and this can prevent the identification of interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference betwee the log and sqrt transformation of a \"big\" value\n",
    "np.sqrt(34565)\n",
    "np.log(34565)\n",
    "\n",
    "# difference betwee the log and sqrt transformation of a \"small\" value\n",
    "np.sqrt(107)\n",
    "np.log(107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below plots the diagnostic plots **QQ Plots** for two sets of variables, like raw (unstransformed) and transformed data, for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_transformations(raw_data, transformed_data, transformation_used):\n",
    "    fig = plt.figure(figsize = (18, 7))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    prob = stats.probplot(raw_data, dist=stats.norm, plot=ax1)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_title('Probplot against the normal distribution (line) ')\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    prob = stats.probplot(transformed_data, dist=stats.norm, plot=ax2)\n",
    "    ax2.set_title('Probplot after ' + transformation_used + ' transformation')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \n",
    "Try **box-cox** (available in **scipy**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data with noise\n",
    "raw_data = stats.loggamma.rvs(14, size=50) + 250\n",
    "\n",
    "# apply box-cox\n",
    "transformed_data, _ = stats.boxcox(raw_data)\n",
    "\n",
    "# plot and compare \n",
    "plot_compare_transformations(raw_data, transformed_data, 'box-cox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sqrt\n",
    "transformed_data, _ = stats.boxcox(df.RM)\n",
    "\n",
    "# plot and compare \n",
    "plot_compare_transformations(df.RM, transformed_data, 'box-cox')\n",
    "\n",
    "# apply sqrt\n",
    "transformed_data = np.log(df.RM)\n",
    "\n",
    "# plot and compare \n",
    "plot_compare_transformations(df.RM, transformed_data, 'log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the same effect in numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with **Shapiro-Wilks's** test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test of normal distribution with Shapiros Test')\n",
    "print('stat:', stats.shapiro(raw_data)[0],'p-value:', stats.shapiro(raw_data)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test of normal distribution with Shapiros Test')\n",
    "print('stat:', stats.shapiro(transformed_data)[0],'p-value:', stats.shapiro(transformed_data)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://www.freepik.com/vectors/data'>Data vector created by stories - www.freepik.com</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
