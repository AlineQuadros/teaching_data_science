{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Predictive modelling with timeseries  </center>\n",
    "# <center> Part 2 -  Exponential Smoothing</center>\n",
    "\n",
    "![Image](images/timeseries.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the two simple forecasts we've seen earlier:\n",
    "* the **naive forecast**  assumes that the most recent observation is the only important one\n",
    "* the **average forecast**  assumes that all past observations are of equal importance  \n",
    "\n",
    "These two methods are extremes, and therefore, simplifications of reality. Most of the times we'll need a model  that is in between.  \n",
    "That's what `exponential smoothing` can do:\n",
    "* gives different weight to all past observations (unlike the average)  \n",
    "...BUT...  \n",
    "* considers that most recent observations should have a relative higher importance (and hence, more weight)\n",
    "\n",
    "\n",
    "### Weights (alpha)  \n",
    "\n",
    "* The degree of smoothing will be given by the parameter `alpha`. \n",
    "* Alpha ranges from 0 to 1  \n",
    "* The sum of the 'alphas' of the entire time series is aprox. equal to 1\n",
    "* The table below illustrates the division of weights depending on the choosen alpha, for a time series of 6 points. When alpha=0.2, the difference between the weight of the latest observation (0.2) and the first observation (0.0655) is only 0.14, meaning that weights are more equally distributed. Now, when alpha = 0.8, the diference is 0.7997, and the latest observations have a much bigger impact in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](images/alpha.png) *Table source: Hyndman and Athanasopoulos. www.otexts.com/fpp2/*\n",
    "\n",
    "\n",
    "### Example:  \n",
    "\n",
    "* The original data is in green.  \n",
    "* The red line shows the forecast using the **lowest alpha**. It makes a broad generalization of the data (almost like an average).\n",
    "* The blue line was produced with an **alpha of 0.5** and captures most of the oscillations.\n",
    "* The pink line has a very **high alpha**, and mostly reproduces the original data but shifting it 1 period in time.\n",
    "* The best forecast is that with the lowest MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](images/es.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic of the exercise \n",
    "\n",
    "We want to visualize and compare the behavior of different `exponential smoothing` methods. \n",
    "\n",
    "1. Load the dataset and split it into train and test data\n",
    "2. Plot only the training data\n",
    "3. Predict the next __ using the baselines shown above\n",
    "4. Plot the baselines\n",
    "5. Add the test data to the plot and compare results:  \n",
    "    a. Which baseline did a better job?  \n",
    "    b. Express the error with numbers. Calculate the MAE, MAPE, and MASE in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# jupyter lab configs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from utils import print_errors\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a nice example for decomposition - production of electrical equipments\n",
    "ele_df = pd.read_csv('datasets/elecequip.csv')\n",
    "ele_df['value'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ele_df.loc[:150, 'value']\n",
    "test = ele_df.loc[150:, 'value']\n",
    "test_start = 150\n",
    "test_end = 194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple exponential smoothing\n",
    "\n",
    "This is a simple method, used when there's no obvious trend or seasonality in the data.  \n",
    "To understand what it does, experiment with the effects of changing `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the effect of alpha\n",
    "\n",
    "Each plot has the same training data but varying levels of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "# plot train data\n",
    "plt.plot(train,  marker='.', color='black')\n",
    "# create different traces to see the effect of changing alpha:\n",
    "fit1 = SimpleExpSmoothing(train).fit(smoothing_level=0.2, optimized=False)\n",
    "plt.plot(fit1.fittedvalues, marker='o', color='blue')\n",
    "plt.show()\n",
    "\n",
    "# create plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "# plot train data\n",
    "plt.plot(train,  marker='.', color='black')\n",
    "# create different traces to see the effect of changing alpha:\n",
    "fit1 = SimpleExpSmoothing(train).fit(smoothing_level=0.5, optimized=False)\n",
    "plt.plot(fit1.fittedvalues, marker='o', color='green')\n",
    "plt.show()\n",
    "\n",
    "# create plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "# plot train data\n",
    "plt.plot(train,  marker='.', color='black')\n",
    "# create different traces to see the effect of changing alpha:\n",
    "fit1 = SimpleExpSmoothing(train).fit(smoothing_level=0.9, optimized=False)\n",
    "plt.plot(fit1.fittedvalues, marker='o', color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential smoothing for trending and/or seasonal data  \n",
    " \n",
    "Most commonly known methods belong to the `Holt-Winter's` family.  \n",
    "These methods can handle the combination of different trends and seasonalities (none, additive, and multiplicative).\n",
    "\n",
    "\n",
    "### In practice \n",
    " \n",
    "In Python, the `Holt-Winter's` family of methods is available in the package `holtwinters` from library `statsmodels`.   \n",
    "You can find the documentation here: https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html.  \n",
    "\n",
    "The method `ExponentialSmoothing` accepts one of {`'add'`, `'mul'`, or `None`} for the parameters `trend` and `seasonality`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seasonal_periods = 30\n",
    "fitted_model = ExponentialSmoothing(train, trend=None, seasonal='add', seasonal_periods=seasonal_periods).fit()\n",
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = fitted_model.forecast(len(test)).rename('Predictions')\n",
    "print_errors(test, test_predictions, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(legend=True,label='train')\n",
    "test.plot(legend=True,label='test',figsize=(10,7))\n",
    "test_predictions.plot(legend=True,label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if you set trend and seasonality to `None` ðŸ¤”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the number of `seasonal_periods` and see what happens with the number of coefficients estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we improve?\n",
    "Try out different trend and seasonality types. I suggest using **BIC** and **AIC** to compare the different models since they will have different complexities, besides using MAE, RMSE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://www.freepik.com/vectors/business'>Business vector created by freepik - www.freepik.com</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
